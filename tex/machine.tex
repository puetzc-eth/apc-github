% Introduction to Machine-Learning (1 week)

%\lstinputlisting[language=C++]{src/intro/test.cpp}

The idea behind \emph{supervised machine learning} is to determine a function from a set of training data that can be used to predict the output of new data. The training data is a representative set of examples with input values (typically a vector) and a known output value. Typical machine learning tasks are, for example, \emph{classifications} (discrete output: for example, a molecule is biologically active or not) or \emph{regressions} (continuous output: for example, linear regression).

\begin{center}\includegraphics[width=0.65\textwidth]{img/machine/MachineGeneralScheme.png}\end{center}

The following methods, among others, can be used for this purpose:

\begin{itemize}
    \item Linear models, e.g. linear regression, logistic regression
    \item Decision tree
    \item Random forest
    \item Gradient tree boosting
    \item Na√Øve Bayes
    \item Support vector machines (SVM)
    \item Artificial neural networks
\end{itemize}

\paragraph{Validation}
Once a function has been determined from the training data, it still needs to be validated, since the training data for such algorithms is often too small and there is a risk of overfitting the training data, whereby the function performs well on the training set but not well on new data. Typical validation techniques are \emph{train/test split} or \emph{K-fold cross-validation}.

\begin{center}\includegraphics[width=0.65\textwidth]{img/machine/MachineOverUnderFiting.png}\end{center}

For classification models, slightly different assessment criteria for the quality of an ML-model must be applied, such as the \emph{receiver-operator curve} (ROC), in which the true positive rate is plotted against the false positive rate. Depending on the error rates, further metrices for classification models can be calculated.

\begin{center}\includegraphics[width=0.45\textwidth]{img/machine/MachineRocCurve.png}\includegraphics[width=0.45\textwidth]{img/machine/MachineClassificationMetrices.png}\end{center}

\paragraph{Unsupervised Machine Learning vs Supervised Machine Learning}
The difference between supervised machine learning and unsupervised machine learning is that in unsupervised machine learning the dataset does not yet have any labels without predefined patterns according to which the data is sorted. An example of this is clustering. In supervised machine learning, training data is used to determine a function.

\subsection{Linear models: linear regression, logistic regression}

\paragraph{Linear Regression}
Linear regression is the simplest ML-model and has as training data a set of example values of $(x,y)$ values, where the input values are the $x$ values and the output values (continues) are the $y$ values. The goal of the regression is to find a linear function $y(x)=ax+b$ for which the squared distances of the points from the regression line are minimized. 

\paragraph{Logistic Regression}
In logistic regression (similar to multiple linear regression), an input vector $X_i=(x_{1,i}, x_{2,i},\cdots,x_{N,i})$ is compared to a discrete output value $Y$ (here either 0 or 1, but generally classification) as training data. The coefficients $(a_1,a_2,\cdots,a_N)$ are fitted for a logistic function, where $p_i$ is the probability for a $y$-value. These models are often relatively simple but very effective.

\begin{align}
    \log_{2}\left(\frac{p_i}{1-p_i}\right)=a_0+a_1x_{1,i}+\cdots+a_Nx_{N,i}
\end{align}

\begin{center}\includegraphics[width=0.35\textwidth]{img/machine/MachineLinearRegression.png}\includegraphics[width=0.55\textwidth]{img/machine/MachineLogisticRegression.png}\end{center}

\subsection{Decision trees}

\emph{Decision trees} can be used in machine learning for both classifications and regressions. They use as training data as input variable vectors with multiple attributes and as output variables integer or continuous values. The attributes of the input vector determine which branch of the decision tree is taken.

\begin{center}\includegraphics[width=0.65\textwidth]{img/machine/MachineDecisionTrees.png}\end{center}

In decision trees, the selection of attributes that are responsible for the splitting at the nodes is crucial. You set up the tree so that you get the maximum amount of information at each decision point, which is synonymous with reducing entropy/impurity. Where $p_{x,i}$ is the probability for the value $i$ for the attribute $x$, $N$ is the number of samples and $n$ is the number of attribute values.

\begin{align}
    &S(x)=\left(-p_{x,A}\log_2p_{x,A}\right)+\left(-p_{x,I}\log_2p_{x,I}\right)&Gain(x_2)=S(x_1)-\sum_{i=1}^{n}\frac{N_{x_2,\mathrm{tot}}}{N_{x_1,\mathrm{tot}}}S(x_2,i)
\end{align}

\begin{center}\includegraphics[width=0.65\textwidth]{img/machine/MachineDecisionTreesEntropy.png}\end{center}

\deff{Gini impurity}{Gini impurity measures how often a randomly chosen element of a set would be incorrectly labeled if it were labeled randomly and independently according to the distribution of labels in the set. It reaches its minimum (zero) when all cases in the node fall into a single target category.}

\begin{center}\includegraphics[width=0.65\textwidth]{img/machine/MachineDecisionTreesEntropyExample.png}\end{center}

The advantages and disadvantages of decision trees for machine learning are listed below:

\begin{itemize}
    \item[$\oplus$] Simple to understand and easy to interpret
    \item[$\oplus$] Little data preparation needed
    \item[$\oplus$] Both classification and regression possible
    \item[$\oplus$] Validation using statistical tests possible: Reliability of the model can be accounted for
    \item[$\ominus$] Prone to overfitting: Over-complex trees that do not generalise well (remedies: Pruning, limit maximum depth)
    \item[$\ominus$] Small variations in the data can lead to completely different trees
    \item[$\ominus$] Problem of learning an optimal decision tree is NP-complete (heuristic (greedy) algorithms required)
    \item[$\ominus$] If the training data is unbalanced, the resulting trees will be biased
\end{itemize}

\subsection{Ensemble methods}

The idea behind \emph{ensemble learning} is that you use the results of many not-so-strong models to achieve a strong overall result, which is then comparable to the wisdom of the crowd. For example, this is the case when several smaller decision trees only have a random subset of the training data and the output is decided by the majority of the individual models. The biggest advantage is that these models are usually more robust and the data that was not used by the individual trees can be used immediately as validation.

\begin{center}\includegraphics[width=0.65\textwidth]{img/machine/MachineEnsemble.png}\end{center}

\subsubsection{Random Forests}

Random Forests is an ensemble method that introduces a second random element (on the one hand, a random subset of training data per tree and, on the other hand, a random subset of attributes per node) to develop even more robust machine learning models. The first application for drug design of RFs was for quantitative structure-activity relationship models.

\begin{center}\includegraphics[width=0.65\textwidth]{img/machine/MachineEnsembleRandomForests.png}\end{center}

\subsubsection{Gradient Tree Boosting}

Gradient tree boosting is an ensemble method that is very similar to random forests, but here the trees are created sequentially after the training data has been run through, so that this information can be used to create better trees and thus make even better models than RFs, but the implementation usually takes longer.

\begin{center}\includegraphics[width=0.65\textwidth]{img/machine/MachineEnsembleGradientTreeBoosting.png}\end{center}

\subsection{Artificial neural networks}

\deff{Artificial neuron}{The artificial neuron is the basic unit of information reception where the inputs are received and multiplied, summed, and processed via a transfer function before being delivered to the output.}

Here $f()$ is the transfer function, $b$ is the external error, the weights $w_{ij}$ are determined by the training and $a()$ is the decision function. This model always requires a training and a validation set.

\begin{align}
    y_i=f\left(\sum_{j=1}^{n}w_{ij}x_j+b\right)
\end{align}

\begin{center}\includegraphics[width=0.65\textwidth]{img/machine/MachineArtificialNeuralNetworks.png}\end{center}

\subsection{Practical Considerations}

