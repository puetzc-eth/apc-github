% Introduction to Machine-Learning (1 week)

%\lstinputlisting[language=C++]{src/intro/test.cpp}

The idea behind \emph{supervised machine learning} is to determine a function from a set of training data that can be used to predict the output of new data. The training data is a representative set of examples with input values (typically a vector) and a known output value. Typical machine learning tasks are, for example, \emph{classifications} (discrete output: for example, a molecule is biologically active or not) or \emph{regressions} (continuous output: for example, linear regression).

\begin{center}\includegraphics[width=0.65\textwidth]{img/machine/MachineGeneralScheme.png}\end{center}

The following methods, among others, can be used for this purpose:

\begin{itemize}
    \item Linear models, e.g. linear regression, logistic regression
    \item Decision tree
    \item Random forest
    \item Gradient tree boosting
    \item Na√Øve Bayes
    \item Support vector machines (SVM)
    \item Artificial neural networks
\end{itemize}

\paragraph{Validation}
Once a function has been determined from the training data, it still needs to be validated, since the training data for such algorithms is often too small and there is a risk of overfitting the training data, whereby the function performs well on the training set but not well on new data. Typical validation techniques are \emph{train/test split} or \emph{K-fold cross-validation}.

\begin{center}\includegraphics[width=0.65\textwidth]{img/machine/MachineOverUnderFiting.png}\end{center}

For classification models, slightly different assessment criteria for the quality of an ML-model must be applied, such as the \emph{receiver-operator curve} (ROC), in which the true positive rate is plotted against the false positive rate. Depending on the error rates, further metrices for classification models can be calculated.

\begin{center}\includegraphics[width=0.45\textwidth]{img/machine/MachineRocCurve.png}\includegraphics[width=0.45\textwidth]{img/machine/MachineClassificationMetrices.png}\end{center}

\paragraph{Unsupervised Machine Learning vs Supervised Machine Learning}
The difference between supervised machine learning and unsupervised machine learning is that in unsupervised machine learning the dataset does not yet have any labels without predefined patterns according to which the data is sorted. An example of this is clustering. In supervised machine learning, training data is used to determine a function.

\subsection{Linear models: linear regression, logistic regression}

\paragraph{Linear Regression}
Linear regression is the simplest ML-model and has as training data a set of example values of $(x,y)$ values, where the input values are the $x$ values and the output values (continues) are the $y$ values. The goal of the regression is to find a linear function $y(x)=ax+b$ for which the squared distances of the points from the regression line are minimized. 

\paragraph{Logistic Regression}
In logistic regression (similar to multiple linear regression), an input vector $X_i=(x_{1,i}, x_{2,i},\cdots,x_{N,i})$ is compared to a discrete output value $Y$ (here either 0 or 1, but generally classification) as training data. The coefficients $(a_1,a_2,\cdots,a_N)$ are fitted for a logistic function, where $p_i$ is the probability for a $y$-value. These models are often relatively simple but very effective.

\begin{align}
    \log_{2}\left(\frac{p_i}{1-p_i}\right)=a_0+a_1x_{1,i}+\cdots+a_Nx_{N,i}
\end{align}

\begin{center}\includegraphics[width=0.35\textwidth]{img/machine/MachineLinearRegression.png}\includegraphics[width=0.55\textwidth]{img/machine/MachineLogisticRegression.png}\end{center}

\subsection{Decision trees}

\emph{Decision trees} can be used in machine learning for both classifications and regressions. They use as training data as input variable vectors with multiple attributes and as output variables integer or continuous values. The attributes of the input vector determine which branch of the decision tree is taken.

\begin{center}\includegraphics[width=0.65\textwidth]{img/machine/MachineDecisionTrees.png}\end{center}

In decision trees, the selection of attributes that are responsible for the splitting at the nodes is crucial. You set up the tree so that you get the maximum amount of information at each decision point, which is synonymous with reducing entropy/impurity. Where $p_{x,i}$ is the probability for the value $i$ for the attribute $x$, $N$ is the number of samples and $n$ is the number of attribute values.

\begin{align}
    &S(x)=\left(-p_{x,A}\log_2p_{x,A}\right)+\left(-p_{x,I}\log_2p_{x,I}\right)&Gain(x_2)=S(x_1)-\sum_{i=1}^{n}\frac{N_{x_2,\mathrm{tot}}}{N_{x_1,\mathrm{tot}}}S(x_2,i)
\end{align}

\begin{center}\includegraphics[width=0.65\textwidth]{img/machine/MachineDecisionTreesEntropy.png}\end{center}

\deff{Gini impurity}{Gini impurity measures how often a randomly chosen element of a set would be incorrectly labeled if it were labeled randomly and independently according to the distribution of labels in the set. It reaches its minimum (zero) when all cases in the node fall into a single target category.}

\begin{center}\includegraphics[width=0.65\textwidth]{img/machine/MachineDecisionTreesEntropyExample.png}\end{center}

The advantages and disadvantages of decision trees for machine learning are listed below:

\begin{itemize}
    \item[$\oplus$] Simple to understand and easy to interpret
    \item[$\oplus$] Little data preparation needed
    \item[$\oplus$] Both classification and regression possible
    \item[$\oplus$] Validation using statistical tests possible: Reliability of the model can be accounted for
    \item[$\ominus$] Prone to overfitting: Over-complex trees that do not generalise well (remedies: Pruning, limit maximum depth)
    \item[$\ominus$] Small variations in the data can lead to completely different trees
    \item[$\ominus$] Problem of learning an optimal decision tree is NP-complete (heuristic (greedy) algorithms required)
    \item[$\ominus$] If the training data is unbalanced, the resulting trees will be biased
\end{itemize}

\subsection{Ensemble methods}

\subsection{Artificial neural networks}

\subsection{Practical considerations}